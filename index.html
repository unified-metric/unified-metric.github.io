<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG" />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta
      property="og:description"
      content="SOCIAL MEDIA DESCRIPTION TAG TAG"
    />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
    <meta
      name="twitter:description"
      content="TWITTER BANNER DESCRIPTION META TAG"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/images/your_twitter_banner_image.png"
    />
    <meta name="twitter:card" content="summary_large_image" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Academic Project Page</title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" /> -->
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                CAS: A Probability-Based Approach for Universal Condition
                Alignment Score
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://sites.google.com/view/chunsanhong" target="_blank"
                    >Chunsan Hong</a
                  ><sup>1*</sup>,</span
                >
                <span class="author-block">
                  <a href="https://sites.google.com/view/byungheecha" target="_blank"
                    >ByungHee Cha</a
                  ><sup>2*</sup>,</span
                >
                <span class="author-block">
                  <a href="https://ami.postech.ac.kr/members/tae-hyun-oh" target="_blank"
                    >Taehyun Oh</a
                  ><sup>3</sup>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"
                  >KAIST<sup>1</sup> &nbsp Seoul National University<sup>2</sup>
                  &nbsp POSTECH<sup>3</sup><br />ICLR 2024 (spotlight)</span
                >
                <span class="eql-cntrb"
                  ><small
                    ><br /><sup>*</sup>Indicates Equal Contribution</small
                  ></span
                >
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a
                      href="https://openreview.net/pdf?id=E78OaH2s3f"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Supplementary PDF link -->
                  <span class="link-block">
                    <a
                      href="https://openreview.net/pdf?id=E78OaH2s3f"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/unified-metric/unified_metric"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/<ARXIV PAPER ID>"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser video-->
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="static/images/intro_v12.svg" alt="MY ALT TEXT" />
          <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
          <!-- Your video here -->
          <!-- <source src="static/videos/banner_video.mp4" type="video/mp4" /> -->
          <!-- </video> -->
          <h2 class="subtitle has-text-centered">
            Our metric demonstrates automatic and reliable assessment of
            perceptual alignment between condition and generated results (i.e.,
            cherry-picking without humans) across text-to-image models,
            diffusion models trained on specific domains such as Van Gogh,
            InstructPix2Pix, ControlNet, and AudioLDM, but not limited to.
          </h2>
        </div>
      </div>
    </section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Recent conditional diffusion models have shown remarkable
                advancements and have been widely applied in fascinating
                real-world applications. However, samples generated by these
                models often do not strictly comply with user-provided
                conditions. Due to this, there have been few attempts to
                evaluate this alignment via pre-trained scoring models to select
                well-generated samples. Nonetheless, current studies are
                confined to the text-to-image domain and require large training
                datasets. This suggests that crafting alignment scores for
                various conditions will demand considerable resources in the
                future. In this context, we introduce a universal condition
                alignment score that leverages the conditional probability
                measurable through the diffusion process. Our technique operates
                across all conditions and requires no additional models beyond
                the diffusion model used for generation, effectively enabling
                self-rejection. Our experiments validate that our metric
                effectively applies in diverse conditional generations, such as
                text-to-image, {instruction, image}-to-image,
                edge/scribble-to-image, and text-to-audio.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- Youtube video -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <!-- Paper video. -->
          <h2 class="title is-3">Contribution</h2>
          <div class="columns is-centered has-text-justified">
            <div class="column">
              <ul>
                <li>We present a universal alignment score applicable to all possible conditions.</li>
                <li> Our self-rejection technique leverages the diffusion model that is used for generating images,
thereby eliminating the dependency on well-curated datasets and separately pre-trained score mod-
els that the previous studies grappled with</li>
                <li>We developed a method for improved DDIM inversion, boosting the performance of our technique</li>
              </ul>
            </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <!-- Paper video. -->
          <h2 class="title is-3">Approach</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column has-text-justified">
              <img src="static/images/mthd.png" alt="MY ALT TEXT" />
              <p>
                We hypothesized that the conditional likelihood is biased by the
                unconditional likelihood and validated this hypothesis through
                experimentation. Consequently, we adopted the difference between
                the conditional and unconditional likelihoods as a metric. The
                likelihood was calculated by applying the instantaneous change
                of variables formula to the DDIM ODE. The formula for CAS is
                given by: \[ CAS(\mathbf{x},\mathbf{c},\theta) = \log
                \frac{p_1(\mathbf{x}^{c}(1)|\mathbf{c})}{p_1(\mathbf{x}^{u}(1))}
                - \int_0^1 \alpha'(t)\frac{\nabla_\mathbf{x} \cdot
                \epsilon_{\theta}(\mathbf{x}^c(t), c, t) - \nabla_\mathbf{x}
                \cdot \epsilon_{\theta}(\mathbf{x}^{u}(t), \emptyset,
                t)}{2\alpha(t)\sqrt{1-\alpha(t)}}dt. \] The calculation of
                \(\nabla_\mathbf{x} \cdot \epsilon_{\theta}(\mathbf{x}^c(t), c,
                t)\) was performed using the Skilling-Hutchinson trace estimator
                \(\mathbb{E}_{p(\mathbf{z})}[\mathbf{z}^\top\nabla_\mathbf{x}
                \epsilon_\theta(\mathbf{x},t)\mathbf{z}]\). To improve inference
                time, instead of calculating the gradient of
                \(\epsilon_{\theta}\) through backward propagation, we
                approximated it using \(\mathbf{z}^\top \nabla_\mathbf{x}
                \epsilon_{\theta}(\mathbf{x}, t) \mathbf{z} \simeq
                \mathbf{z}^\top\frac{\epsilon_{\theta}(\mathbf{x}+\sigma
                \mathbf{z}, t)-\epsilon_{\theta}(\mathbf{x}, t)}{\sigma} =
                \frac{1}{\sigma}
                \mathbf{z}^\top(\epsilon_{\theta}(\mathbf{x}+\sigma \mathbf{z},
                t) - \epsilon_{\theta}(\mathbf{x}, t))\). Additionally, we
                utilized the Simpsons 3/8 rule in the integration process.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <!-- Paper video. -->
          <h2 class="title is-3">Results</h2>
          <div class="is-centered has-text-centered">
            <div class="column has-text-justified">
              <!-- <img src="static/images/pickscore_result.png" alt="MY ALT TEXT" /> -->
              <p>
                Our experiments demonstrated the Conditional Alignment Score
                (CAS)'s effectiveness across five distinct models, with a
                particular focus on the T2I diffusion model fine-tuned in the
                Van Gogh domain. CAS outperformed established baselines such as
                CLIP Score, Human Preference Score, Image Reward, and Pick Score
                by a significant margin, especially notable in the Van Gogh
                dataset where it showed approximately 7.4% higher accuracy than
                the next best method. This success underscores CAS's unique
                ability to accurately measure style fidelity and text-to-image
                (T2I) alignment without requiring additional model training. Our
                method's robustness is further highlighted through experiments
                utilizing negative weighting on prompts, effectively
                demonstrating CAS's precision in evaluating domain-specific
                styles. Remarkably, CAS achieved this superior performance
                without relying on the extensive training datasets necessary for
                other metrics, indicating its efficiency and potential for
                broader application in generative modeling.
              </p>
            </div>
            <div class="column has-text-justified">
              <img
                src="static/images/multimodal_result.png"
                alt="MY ALT TEXT"
              />
              <p>
                Extending our evaluation to additional modalities, including
                InstructPix2Pix, ControlNet, and AudioLDM, reaffirmed CAS's
                universal applicability. Human preference evaluations across
                these modalities revealed a consistent preference for outputs
                ranked highly by CAS, indicating its alignment with human
                judgment even in ambiguous conditions. Specifically, the
                experiments showed over 60% preference accuracy for CAS across
                different input-output pairs, signifying its effectiveness in
                capturing the nuanced preferences of human evaluators. The
                results from modalities beyond image generation, such as
                text-to-audio with AudioLDM, further validated the metric's
                versatility. Additionally, in the context of InstructPix2Pix,
                CAS's correlation with directional CLIP similarity provided
                empirical evidence of its reliability in measuring alignment,
                showcasing its capacity to serve as a comprehensive evaluation
                tool for a wide range of diffusion models.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End youtube video -->

    <!-- Video carousel -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <h2 class="title">Examples</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/result_1.png" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                Text-to-image sample result
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/result_2.png" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                Text-to-image sample result
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/result_3.png" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                {Image, Instruction}-to-image sample result
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/result_4.png" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                {Image, Instruction}-to-image sample result
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/result_5.png" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                {Canny Edge, Prompt}-to-image sample result
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/result_6.png" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                {Canny Edge, Prompt}-to-image sample result
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/result_7.png" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                {Scribble, Prompt}-to-image sample result
              </h2>
            </div>
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/result_8.png" alt="MY ALT TEXT" />
              <h2 class="subtitle has-text-centered">
                {Scribble, Prompt}-to-image sample result
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End video carousel -->

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{
  hong2024CAS,
  title={CAS: A Probability-based Approach for Universal Condition Alignment Score},
  author={Chunsan Hong and Byunghee Cha and Tae-Hyun Oh},
  booktitle={International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=E78OaH2s3f}
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                > project page. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>
